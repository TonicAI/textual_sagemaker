{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a08a942-5061-4f38-99a1-988091172614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tonic Textual\n",
    "\n",
    "[Tonic Textual](https://textual.tonic.ai) makes training data safe for production by removing customer PII/PHI from your model weights.  It does this by sanitizing and synthesizing your data prior to model training using our state of the art named entity recognition and entity linking models.  This notebook will help illustate how you can use Textual to sanitize unstructured text.  It will also show how you can access your data sitting in your AWS Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3660d-32ac-4448-8ae0-6e1221a97f20",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Before we start accessing data in AWS lets get a feel for how the SDK works.  We'll start by redacting and synthesizing some simple pieces of text.  To get started, you'll first need to create a Textual API key.  You can do that by creating a *free* account at [https://textual.tonic.ai/signup](https://textual.tonic.ai/signup).  Once you've created your account create an API key from the top-navbar.  \n",
    "\n",
    "For this tutorial, you can hard code your API key value or use the below code snippet to access your API key stored in AWS Secrets Manager.  If you use AWS Secrets manager you'll need to provide your SageMaker IAM role with permission to secretsmanager:GetSecretValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52382250-c8f2-4db9-bb83-e4247302950c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:25:36.228286Z",
     "iopub.status.busy": "2025-09-25T02:25:36.227993Z",
     "iopub.status.idle": "2025-09-25T02:25:37.793510Z",
     "shell.execute_reply": "2025-09-25T02:25:37.792696Z",
     "shell.execute_reply.started": "2025-09-25T02:25:36.228262Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tonic_textual\n",
    "from tonic_textual.redact_api import TextualNer\n",
    "import json\n",
    "\n",
    "textual_api_key='<Tonic Textual API key goes here>'\n",
    "textual_ner = TextualNer(api_key=textual_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320af79-d4db-47ba-a38b-7ab0494ecdc1",
   "metadata": {},
   "source": [
    "Now lets redact and synthesize some text using the SDK.  The response from the **redact()** call returns both the sanitized text as well as a list of all found entities.  In the below example, we'll proces the following text:\n",
    "\n",
    "> The patient, John Smith is a 42-year male and suffers from Crohn\\'s Disease.\n",
    "\n",
    "And we'll find these entities:\n",
    "\n",
    "- **John** - A first name\n",
    "- **Smith** - A last name\n",
    "- **42** - A person's age\n",
    "- **male** - A persons gender\n",
    "\n",
    "The generated redacted text will be\n",
    "\n",
    "> The patient, [NAME_GIVEN_dySb5] [NAME_FAMILY_7w4Db3] is a [PERSON_AGE_BF3]-year [GENDER_IDENTIFIER_AzwE0] and suffers from Wilson's Disease.\n",
    "\n",
    "By default, Textual will flag all possible found entities and will replace them with redacted tokens, as you can see above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ece2518-fe55-485a-abbb-e6fcb5e97de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:35:18.406797Z",
     "iopub.status.busy": "2025-09-25T02:35:18.406538Z",
     "iopub.status.idle": "2025-09-25T02:35:18.509276Z",
     "shell.execute_reply": "2025-09-25T02:35:18.508491Z",
     "shell.execute_reply.started": "2025-09-25T02:35:18.406778Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"original_text\": \"The patient, John Smith is a 42-year male and suffers from Crohn's Disease.\",\n",
      "    \"redacted_text\": \"The patient, [NAME_GIVEN_dySb5] [NAME_FAMILY_7w4Db3] is a [PERSON_AGE_BF3]-year [GENDER_IDENTIFIER_AzwE0] and suffers from [NAME_FAMILY_2lZ9KI]'s Disease.\",\n",
      "    \"usage\": 13,\n",
      "    \"de_identify_results\": [\n",
      "        {\n",
      "            \"start\": 13,\n",
      "            \"end\": 17,\n",
      "            \"new_start\": 13,\n",
      "            \"new_end\": 31,\n",
      "            \"label\": \"NAME_GIVEN\",\n",
      "            \"text\": \"John\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"[NAME_GIVEN_dySb5]\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 18,\n",
      "            \"end\": 23,\n",
      "            \"new_start\": 32,\n",
      "            \"new_end\": 52,\n",
      "            \"label\": \"NAME_FAMILY\",\n",
      "            \"text\": \"Smith\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"[NAME_FAMILY_7w4Db3]\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 29,\n",
      "            \"end\": 31,\n",
      "            \"new_start\": 58,\n",
      "            \"new_end\": 74,\n",
      "            \"label\": \"PERSON_AGE\",\n",
      "            \"text\": \"42\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"[PERSON_AGE_BF3]\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 37,\n",
      "            \"end\": 41,\n",
      "            \"new_start\": 80,\n",
      "            \"new_end\": 105,\n",
      "            \"label\": \"GENDER_IDENTIFIER\",\n",
      "            \"text\": \"male\",\n",
      "            \"score\": 1,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"[GENDER_IDENTIFIER_AzwE0]\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 59,\n",
      "            \"end\": 64,\n",
      "            \"new_start\": 123,\n",
      "            \"new_end\": 143,\n",
      "            \"label\": \"NAME_FAMILY\",\n",
      "            \"text\": \"Crohn\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"[NAME_FAMILY_2lZ9KI]\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = textual_ner.redact('The patient, John Smith is a 42-year male and suffers from Crohn\\'s Disease.')\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f69d4e-f4ad-4378-baed-c4a8394a336d",
   "metadata": {},
   "source": [
    "Lets now modify the above **redact()** call to instead synthesize values instead of tokenize.  We'll also disable the GENDER_IDENTIFIER entity to show how that is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fcfcf2-52ae-4b04-8897-a340b4256228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:08:22.650731Z",
     "iopub.status.busy": "2025-09-25T02:08:22.650464Z",
     "iopub.status.idle": "2025-09-25T02:08:22.720779Z",
     "shell.execute_reply": "2025-09-25T02:08:22.719879Z",
     "shell.execute_reply.started": "2025-09-25T02:08:22.650703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"original_text\": \"The patient, John Smith is a 42-year male and suffers from Crohn's Disease.\",\n",
      "    \"redacted_text\": \"The patient, Alfonzo Uva is a 47-year male and suffers from Vasconez's Disease.\",\n",
      "    \"usage\": 13,\n",
      "    \"de_identify_results\": [\n",
      "        {\n",
      "            \"start\": 13,\n",
      "            \"end\": 17,\n",
      "            \"new_start\": 13,\n",
      "            \"new_end\": 20,\n",
      "            \"label\": \"NAME_GIVEN\",\n",
      "            \"text\": \"John\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"Alfonzo\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 18,\n",
      "            \"end\": 23,\n",
      "            \"new_start\": 21,\n",
      "            \"new_end\": 24,\n",
      "            \"label\": \"NAME_FAMILY\",\n",
      "            \"text\": \"Smith\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"Uva\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 29,\n",
      "            \"end\": 31,\n",
      "            \"new_start\": 30,\n",
      "            \"new_end\": 32,\n",
      "            \"label\": \"PERSON_AGE\",\n",
      "            \"text\": \"42\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"47\"\n",
      "        },\n",
      "        {\n",
      "            \"start\": 59,\n",
      "            \"end\": 64,\n",
      "            \"new_start\": 60,\n",
      "            \"new_end\": 68,\n",
      "            \"label\": \"NAME_FAMILY\",\n",
      "            \"text\": \"Crohn\",\n",
      "            \"score\": 0.9,\n",
      "            \"language\": \"en\",\n",
      "            \"new_text\": \"Vasconez\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Lets define our list of entities we consider to be sensitive.\n",
    "\n",
    "response = textual_ner.redact(\n",
    "    'The patient, John Smith is a 42-year male and suffers from Crohn\\'s Disease.',\n",
    "    generator_default='Synthesis',\n",
    "    generator_config={'GENDER_IDENTIFIER':'Off'}\n",
    ")\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0435c-0008-42e5-87f7-7f33a8987bac",
   "metadata": {},
   "source": [
    "# Loading data into a AWS Lakehouse Catalog\n",
    "\n",
    "Let's now perform the same sanitization process for some data sitting in your AWS Lakehouse.  We'll store the data in a Lakehouse table and process it with PySpark.  We'll start by fetching a CSV of call transcripts.  Before running the below, make sure you upload the sample transcript CSV into your projects s3 root (can be found via project.s3.root).  The sample CSV can be downloaded from [our Github repo](https://github.com/TonicAI/textual_sagemaker).  The file is called **customer_support_transcripts.parquet**.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "A few IAM polices must first be attached to your sagemaker user IAM role.  To find your IAM role you can run\n",
    "\n",
    "```python\n",
    "from sagemaker_studio import Project\n",
    "\n",
    "project = Project()\n",
    "project.iam_role\n",
    "```\n",
    "The following permissions must be attached to the role:\n",
    "\n",
    "- Glue::GetTable\n",
    "- S3::GetObject\n",
    "- S3::PutObject\n",
    "\n",
    "The S3 objects can be scoped to your sagemaker project bucket, which can be found by running\n",
    "\n",
    "```python\n",
    "project.s3.root\n",
    "```\n",
    "\n",
    "With your IAM policies updated for your role, you'll now need to upload our sample call transcript CSV to your sagemaker S3 bucket.  You can do this using Python's boto3 library or directly in the Sagemaker by navigating to S3 on the Sagemaker data tab, going to your bucket, and uploading CSV to the dev/ folder.  The CSV itself can be found here for download: PUT CSV HERE\n",
    "\n",
    "## Loading the data into Lakehouse\n",
    "\n",
    "Let's now take our CSV, sitting in S3, and create a Lakehouse database and table to store the data.  We'll do that below.\n",
    "\n",
    "Before that let's configure our Spark cluster with appropriate resources - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1ed2441-91be-4108-8a05-303838ef1804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:30:37.444186Z",
     "iopub.status.busy": "2025-09-25T02:30:37.443924Z",
     "iopub.status.idle": "2025-09-25T02:32:12.379239Z",
     "shell.execute_reply": "2025-09-25T02:32:12.378516Z",
     "shell.execute_reply.started": "2025-09-25T02:30:37.444166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping session for project.spark.compatibility. Session id: 6b7asobqqa1q5j-55409364-648b-4c27-8b71-569dd080f54b\n",
      "Session stopped.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The following configurations have been updated: {'number_of_workers': 2, 'worker_type': 'G.1X', 'conf': {'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Glue session...\n",
      "Create session for connection: project.spark.compatibility\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Session 6b7asobqqa1q5j-a0568dea-ae30-4097-98a8-02050c0c5ffb has been created.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <table class=\"session_info_table\" style=\"width: 75%; margin-top: var(--jp-content-heading-margin-top); margin-bottom:var(--jp-content-heading-margin-bottom); border: var(--jp-border-width) solid var(--jp-border-color2);\">\n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Id</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Spark UI</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Driver logs</th>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td class=\"application_id\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\">6b7asobqqa1q5j-a0568dea-ae30-4097-98a8-02050c0c5ffb</td>\n",
       "                        <td class=\"spark_ui_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-543337415716-us-east-1-05bc2f6baf22/dzd_b63q0xem4lb3dz/6b7asobqqa1q5j/dev/glue/glue-spark-events-logs/\">link</a></td>\n",
       "                        <td class=\"driver_log_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-543337415716-us-east-1-05bc2f6baf22/dzd_b63q0xem4lb3dz/6b7asobqqa1q5j/dev/glue/glue-spark-system-logs/6b7asobqqa1q5j-a0568dea-ae30-4097-98a8-02050c0c5ffb/driver/stderr.gz\">link</a></td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure --name project.spark.compatibility -f\n",
    "{\n",
    "  \"number_of_workers\": 2,\n",
    "  \"worker_type\": \"G.1X\",\n",
    "  \"conf\": {\n",
    "    \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "    \"spark.pyspark.virtualenv.type\":   \"native\",\n",
    "    \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffc15c4d-4300-4d9f-bf96-a8567667d224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:44:01.823460Z",
     "iopub.status.busy": "2025-09-25T02:44:01.823211Z",
     "iopub.status.idle": "2025-09-25T02:44:34.544780Z",
     "shell.execute_reply": "2025-09-25T02:44:34.543994Z",
     "shell.execute_reply.started": "2025-09-25T02:44:01.823440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-09-25 02:44:02.491780 | Run duration : 0:00:32.050194s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "from sagemaker_studio import Project\n",
    "\n",
    "project = Project()\n",
    "catalog = project.connection().catalog()\n",
    "project_database = catalog.databases[0].name\n",
    "db_name = project_database\n",
    "\n",
    "tbl_name  = \"call_transcripts\"\n",
    "data_path = f\"s3://{project.s3.root.split('/',3)[2]}/lakehouse/{db_name}/{tbl_name}\"\n",
    "\n",
    "spark.sql(f\"\"\"DROP TABLE IF EXISTS {db_name}.{tbl_name}\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{tbl_name} (\n",
    "    id STRING,\n",
    "    customer_id STRING,\n",
    "    transcript STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION '{data_path}'\n",
    "\"\"\")\n",
    "\n",
    "data_path = f\"{project.s3.root}/customer_support_transcripts.parquet\"\n",
    "raw_df = spark.read.parquet(data_path)\n",
    "raw_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(f\"{db_name}.{tbl_name}\")# Enter your code at the start of this line to replace this comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4ed65-c7bf-46fa-b6a6-173812f3da80",
   "metadata": {},
   "source": [
    "# Moving to scale with PySpark\n",
    "\n",
    "With data now in our Lakehouse lets use Spark (PySpark) to santize unstructured text at scale.  Let's get a few things setup first.\n",
    "\n",
    "- We need to install the tonic_textual Python package on the Spark Workers\n",
    "- We need to pass our API key to our Spark nodes so Textual can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a361472-bf0a-4680-8aa8-30c11a49701f",
   "metadata": {},
   "source": [
    "## Install tonic_texutal on your Spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bddbb2f-1f2e-441a-9c8c-0989dce7f421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:36:13.227093Z",
     "iopub.status.busy": "2025-09-25T02:36:13.226921Z",
     "iopub.status.idle": "2025-09-25T02:37:19.513475Z",
     "shell.execute_reply": "2025-09-25T02:37:19.512623Z",
     "shell.execute_reply.started": "2025-09-25T02:36:13.227076Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tonic_textual\n",
      "  Downloading tonic_textual-3.13.0-py3-none-any.whl (92 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.7/92.7 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting more-itertools<11.0.0,>=10.2.0\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.7/69.7 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting requests<3.0.0,>=2.32.3\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 8.8 MB/s eta 0:00:00\n",
      "Collecting tqdm<5.0.0,>=4.67.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 18.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib64/python3.11/site-packages (from requests<3.0.0,>=2.32.3->tonic_textual) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->tonic_textual) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->tonic_textual) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->tonic_textual) (2024.7.4)\n",
      "Installing collected packages: tqdm, requests, more-itertools, tonic_textual\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.2\n",
      "    Not uninstalling requests at /usr/local/lib/python3.11/site-packages, outside environment /tmp/spark-83a24986-0983-4501-8f6d-01a605edcaea\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed more-itertools-10.8.0 requests-2.32.5 tonic_textual-3.13.0 tqdm-4.67.1\n",
      "\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "amzn-awsgluelibs 5.0.714 requires boto3-stubs==1.34.131, which is not installed.\n",
      "amzn-awsgluelibs 5.0.714 requires mockito==1.5.0, which is not installed.\n",
      "amzn-awsgluelibs 5.0.714 requires pyspark==3.5.2, which is not installed.\n",
      "amzn-awsgluelibs 5.0.714 requires pytest-integration-mark==0.2.0, which is not installed.\n",
      "amzn-awsgluelibs 5.0.714 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: /tmp/spark-83a24986-0983-4501-8f6d-01a605edcaea/bin/python -m pip install --upgrade pip\n",
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-09-25 02:36:13.230596 | Run duration : 0:01:06.280153s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "sc.install_pypi_package(\"tonic_textual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a544847-611e-4567-bb21-722341fb4f02",
   "metadata": {},
   "source": [
    "## Let's send our Textual API key to our spark workers, so its available to our UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a71bcc4d-5fb5-4ef0-b901-6bef1895da19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:38:33.863927Z",
     "iopub.status.busy": "2025-09-25T02:38:33.863656Z",
     "iopub.status.idle": "2025-09-25T02:38:39.296037Z",
     "shell.execute_reply": "2025-09-25T02:38:39.295295Z",
     "shell.execute_reply.started": "2025-09-25T02:38:33.863904Z"
    }
   },
   "outputs": [],
   "source": [
    "%send_to_remote --name project.spark.compatibility --language python --local textual_api_key --remote textual_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f7323-a2e0-4d89-a894-d582294377fe",
   "metadata": {},
   "source": [
    "# Running our sanitization\n",
    "\n",
    "With everything in place, we can now create our PySpark UDF which just wraps the Texutal *redact()* method in a way such that it is call-able from Spark.  We'll create our UDF, and apply it on the 'text' column of our call_transcripts table.\n",
    "\n",
    "## Let's first create our UDF\n",
    "\n",
    "This UDF will synthesize all sensitive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10dbb87a-8046-4c96-9049-733f897c315f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T02:49:37.168428Z",
     "iopub.status.busy": "2025-09-25T02:49:37.168199Z",
     "iopub.status.idle": "2025-09-25T02:49:44.894525Z",
     "shell.execute_reply": "2025-09-25T02:49:44.893751Z",
     "shell.execute_reply.started": "2025-09-25T02:49:37.168408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-09-25 02:49:37.170685 | Run duration : 0:00:07.721229s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "from tonic_textual.redact_api import TextualNer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(StringType())\n",
    "def redact(txt):\n",
    "    if txt is None:\n",
    "        return None\n",
    "\n",
    "    if not hasattr(redact, \"_ner\"):\n",
    "        redact._ner = TextualNer(\"https://textual.tonic.ai\",textual_api_key)\n",
    "\n",
    "    #for a list of all allowed entities go here: https://docs.tonic.ai/textual/tonic-textual-models-about/built-in-entity-types\n",
    "    #or you can view all supported values via the PiiType enum (from tonic_textual.enums.pii_type import PiiType)\n",
    "    sensitive_entities=['NAME_GIVEN','NAME_FAMILY','LOCATION_ADDRESS','LOCATION_CITY','LOCATION_STATE','LOCATION_ZIP','EMAIL_ADDRESS','US_SSN','CVV','CC_EXP','NUMERIC_PII', 'ORGANIZATION']\n",
    "    config = {k: 'Synthesis' for k in sensitive_entities}\n",
    "    \n",
    "    return redact._ner.redact(txt, generator_default='Off', generator_config=config).redacted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251b433-9d13-473a-b679-6579ba91e5f4",
   "metadata": {},
   "source": [
    "## Let's now run the UDF and create a new dataframe with synthetic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bb51c26-4f1a-4ec9-8f64-5c0a5ea4af30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T03:04:35.625687Z",
     "iopub.status.busy": "2025-09-25T03:04:35.625195Z",
     "iopub.status.idle": "2025-09-25T03:04:43.521509Z",
     "shell.execute_reply": "2025-09-25T03:04:43.520721Z",
     "shell.execute_reply.started": "2025-09-25T03:04:35.625660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-09-25 03:04:35.627627 | Run duration : 0:00:07.891162s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "spark.sql(f\"\"\"use {db_name};\"\"\")\n",
    "df = spark.read.table('call_transcripts')\n",
    "\n",
    "df_with_redacted = df.withColumn(\"transcript_redacted\", redact(df[\"transcript\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb8a25-0fa7-4a18-9149-d8e7b835a856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T01:24:33.795634Z",
     "iopub.status.busy": "2025-08-23T01:24:33.795363Z",
     "iopub.status.idle": "2025-08-23T01:24:36.173919Z",
     "shell.execute_reply": "2025-08-23T01:24:36.173106Z",
     "shell.execute_reply.started": "2025-08-23T01:24:33.795612Z"
    }
   },
   "source": [
    "# Now let's create a table with synthetic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48b0eeac-0b64-4aa2-adb9-d0c29e4ab320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T03:06:30.939806Z",
     "iopub.status.busy": "2025-09-25T03:06:30.939585Z",
     "iopub.status.idle": "2025-09-25T03:07:00.763473Z",
     "shell.execute_reply": "2025-09-25T03:07:00.762741Z",
     "shell.execute_reply.started": "2025-09-25T03:06:30.939787Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-09-25 03:06:30.941714 | Run duration : 0:00:29.819098s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "tbl_name  = \"call_transcripts_synthetic\"\n",
    "data_path = f\"s3://{project.s3.root.split('/',3)[2]}/lakehouse/{db_name}/{tbl_name}\"\n",
    "\n",
    "spark.sql(f\"\"\"DROP TABLE IF EXISTS {db_name}.{tbl_name}\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{tbl_name} (\n",
    "    id STRING,\n",
    "    customer_id STRING,\n",
    "    transcript STRING,\n",
    "    transcript_redacted STRING\n",
    ")\n",
    "USING iceberg\n",
    "LOCATION '{data_path}'\n",
    "\"\"\")\n",
    "\n",
    "# data_path = f\"{project.s3.root}/call_transcripts.csv\"\n",
    "df_with_redacted.write.format(\"iceberg\").mode(\"append\").saveAsTable(f\"{db_name}.{tbl_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a34ff5-4e40-40cc-8a3b-b638239e0a65",
   "metadata": {},
   "source": [
    "# Publish the data and make it discoverable\n",
    "\n",
    "The redacted data is now available for querying in your query editor.\n",
    "\n",
    "To publish the data to your SageMaker Catalog, please follow these steps:\n",
    "\n",
    "1. Navigate to the 'Project Overview' of your current project.\n",
    "2. Select 'Data Sources'.\n",
    "3. Look for a source with the connection type 'project.default_lakehouse'. Click 'Run' under the 'Actions' column to make your tables available as assets.\n",
    "4. Navigate to 'Assets'. You should now see both tables: call_transcripts and call_transcripts_synthetic.\n",
    "5. Select the call_transcripts_synthetic table. You'll see options to generate descriptions and accept automated metadata generation. Review and edit the metadata and descriptions as needed.\n",
    "6. When ready, click 'Publish Asset' in the top right corner.\n",
    "7. The asset is now published and discoverable by users outside of the publishing project.\n",
    "8. Project members can review and approve subscription requests from users in other projects.\n",
    "\n",
    "# Subscribe to the Project \n",
    "\n",
    "Users in different projects can follow these steps to subscribe and access the data:\n",
    "\n",
    "1. Log into the domain \n",
    "2. From the home page, select 'Catalog' and then 'Browse assets'\n",
    "3. Browse the available assets. Click on the desired table name to view its descriptions and details\n",
    "4. Click 'Subscribe' in the top right corner\n",
    "5. Provide a detailed reason for your request and click the 'Request' button\n",
    "6. Once your subscription request is approved, you can begin querying the data\n",
    "\n",
    "This approach effectively protects PII information while making relevant data accessible to different teams within your organization. The data can be used for various purposes, including research, sentiment analysis, and gathering customer experience insights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
